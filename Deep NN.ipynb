{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Multi-Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "---\n",
    "Firstly, weights need to be initialized for different layers. Note that in general, the input is not considered as a layer, but output is.\n",
    "\n",
    "For `lth` layer, \n",
    "- weight $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$\n",
    "- bias $b^{[l]}$ has shape $(n^{[l]}, 1)$\n",
    "\n",
    "where $n^{[0]}$ equals the number input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layers_dim):\n",
    "    params = {}\n",
    "    \n",
    "    n = len(layers_dim)\n",
    "    for i in range(1, n):\n",
    "        params['W' + str(i)] = np.random.randn(layers_dim[i], layers_dim[i-1])*0.01\n",
    "        params['b' + str(i)] = np.zeros((layers_dim[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.00197904, -0.01213369],\n",
       "        [ 0.00689108,  0.00453008],\n",
       "        [-0.00691454,  0.01541786],\n",
       "        [-0.01061402,  0.00787606],\n",
       "        [ 0.01147524,  0.00290551]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.00037371, -0.0026616 ,  0.00046249,  0.00950304,  0.01676771]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = weights_init([2, 5, 1])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "## Equations of Multi-layer\n",
    "---\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) $$\n",
    "\n",
    "Where $l$ is the `lth` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23147522 0.11920292 0.78583498] [0.  0.  1.3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1.2, -2.0, 1.3])\n",
    "\n",
    "sx = sigmoid(x)\n",
    "rx = relu(x)\n",
    "\n",
    "print(sx, rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    # intermediate layer use relu as activation\n",
    "    # last layer use sigmoid\n",
    "    n_layers = int(len(params)/2)\n",
    "    A = X\n",
    "    cache = {}\n",
    "    for i in range(1, n_layers):\n",
    "        W, b = params['W'+str(i)], params['b'+str(i)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "    \n",
    "    # last layer\n",
    "    W, b = params['W'+str(i+1)], params['b'+str(i+1)]\n",
    "    Z = np.dot(W, A) + b\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(i+1)] = Z\n",
    "    cache['A'+str(i+1)] = A\n",
    "    \n",
    "    return cache, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1., 1.]).reshape(2, 1)\n",
    "cache, A = forward(X, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[-0.01411272],\n",
       "        [ 0.01142116],\n",
       "        [ 0.00850332],\n",
       "        [-0.00273796],\n",
       "        [ 0.01438075]]),\n",
       " 'A1': array([[0.        ],\n",
       "        [0.01142116],\n",
       "        [0.00850332],\n",
       "        [0.        ],\n",
       "        [0.01438075]]),\n",
       " 'Z2': array([[0.00021467]]),\n",
       " 'A2': array([[0.50005367]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50005367]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "---\n",
    "Still we consider this a binary classification, the cost of a batch would be:\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n",
    "\n",
    "Where $a$ is the predicted value, and $y$ is the actual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23101772979827936\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0.9, 0.3]])\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "loss = compute_cost(A, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "---\n",
    "<img src='images/backprop_kiank.png' style=\"width:800px;height:250px;\">\n",
    "<caption><center> **[source]**: https://github.com/enggen/Deep-Learning-Coursera </center></caption>\n",
    "\n",
    "The backward gradient can be calculated in recurrent fashion:\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) $$\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implementation of derivative of `sigmoid` and `relu` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(A, Z):\n",
    "    grad = np.multiply(A, 1-A)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def relu_grad(A, Z):\n",
    "    grad = np.zeros(Z.shape)\n",
    "    grad[Z>0] = 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87901144 0.29881771]\n",
      " [1.68253349 2.73842285]\n",
      " [1.97877652 0.12396486]] \n",
      "\n",
      "[[0.70661733 0.57415347]\n",
      " [0.84323972 0.93925618]\n",
      " [0.87855068 0.53095159]] \n",
      "\n",
      "[[0.20730928 0.24450126]\n",
      " [0.1321865  0.05705401]\n",
      " [0.10669938 0.249042  ]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = sigmoid(z)\n",
    "g = sigmoid_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63378157  1.22440607]\n",
      " [-0.94572088  0.34021284]\n",
      " [-0.14649618  0.22595908]] \n",
      "\n",
      "[[0.63378157 1.22440607]\n",
      " [0.         0.34021284]\n",
      " [0.         0.22595908]] \n",
      "\n",
      "[[1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = relu(z)\n",
    "g = relu_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the equations above, we have our implementation of backward propagation. Note that except the last layer where `sigmoid` function is used, the rest we all apply `relu` derivative to get the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    params: weight [W, b]\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "        \n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_grad(A, Z))\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW2': array([[ 0.        , -0.00570997, -0.0042512 ,  0.        , -0.00718961]]),\n",
       " 'db2': array([[-0.49994633]]),\n",
       " 'dW1': array([[ 0.        ,  0.        ],\n",
       "        [ 0.00133066,  0.00133066],\n",
       "        [-0.00023122, -0.00023122],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [-0.00838296, -0.00838296]]),\n",
       " 'db1': array([[ 0.        ],\n",
       "        [ 0.00133066],\n",
       "        [-0.00023122],\n",
       "        [ 0.        ],\n",
       "        [-0.00838296]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = backward(p, cache, np.array([[1], [1]]), np.array([[1]]))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the gradients, we have our weights updated as following:\n",
    "    \n",
    "$$ W^{[l]} -= dW^{[l]} $$\n",
    "$$ b^{[l]} -= db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, lr):\n",
    "    n_layers = int(len(params)/2)\n",
    "    for i in range(1, n_layers+1):\n",
    "        dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "        params['W'+str(i)] -= lr*dW\n",
    "        params['b'+str(i)] -= lr*db\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.00197904, -0.01213369],\n",
       "        [ 0.00556042,  0.00319942],\n",
       "        [-0.00668332,  0.01564908],\n",
       "        [-0.01061402,  0.00787606],\n",
       "        [ 0.0198582 ,  0.01128847]]),\n",
       " 'b1': array([[ 0.        ],\n",
       "        [-0.00133066],\n",
       "        [ 0.00023122],\n",
       "        [ 0.        ],\n",
       "        [ 0.00838296]]),\n",
       " 'W2': array([[-0.00037371,  0.00304836,  0.00471369,  0.00950304,  0.02395732]]),\n",
       " 'b2': array([[0.49994633]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = optimize(p, g, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply on Dataset\n",
    "---\n",
    "Let's have our model apply on created dataset with 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    \n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.round(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, layers: list, batch_size=200, n_iter=100, lr=0.1):\n",
    "    # prepare batch training\n",
    "    batches = generate_batch(X_train, batch_size)\n",
    "    # init weights\n",
    "    parameters = weights_init(layers)\n",
    "    for i in range(n_iter):\n",
    "        for batch in batches:\n",
    "            X = X_train[batch, :].T\n",
    "            Y = y_train[batch].reshape(1, -1)\n",
    "            cache, A = forward(X, parameters)\n",
    "            grads = backward(parameters, cache, X, Y)\n",
    "            parameters = optimize(parameters, grads, lr)\n",
    "            \n",
    "        if i%10 == 0:\n",
    "            loss = compute_cost(A, Y)\n",
    "            print(f'iteration {i}: loss {loss}')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 3-layer neural network, with input of 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6930831830164655\n",
      "iteration 10: loss 0.6930082822907631\n",
      "iteration 20: loss 0.6929949487286129\n",
      "iteration 30: loss 0.6929543341306021\n",
      "iteration 40: loss 0.6927636568599188\n",
      "iteration 50: loss 0.690073352055835\n",
      "iteration 60: loss 0.2531001812337807\n",
      "iteration 70: loss 0.127696331048521\n",
      "iteration 80: loss 0.08193633942165292\n",
      "iteration 90: loss 0.05580582920505571\n"
     ]
    }
   ],
   "source": [
    "trained_param = train(X_train, y_train, layers=[200, 20, 10, 1], batch_size=200, n_iter=100, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, pred = forward(X_test.T, trained_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.39999999999999%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.params = {}\n",
    "       \n",
    "    \n",
    "    def weights_init(self):\n",
    "        n = len(self.layers)\n",
    "        for i in range(1, n):\n",
    "            self.params['W' + str(i)] = np.random.randn(self.layers[i], self.layers[i-1])*0.01\n",
    "            self.params['b' + str(i)] = np.zeros((self.layers[i], 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cost(A, Y):\n",
    "        \"\"\"\n",
    "        For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "        \"\"\"\n",
    "        assert A.shape == Y.shape\n",
    "        m = A.shape[1]\n",
    "        s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "        loss = -s/m\n",
    "        return np.squeeze(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_grad(A, Z):\n",
    "        grad = np.multiply(A, 1-A)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_grad(A, Z):\n",
    "        grad = np.zeros(Z.shape)\n",
    "        grad[Z>0] = 1\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # intermediate layer use relu as activation\n",
    "        # last layer use sigmoid\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        A = X\n",
    "        cache = {}\n",
    "        for i in range(1, n_layers):\n",
    "            W, b = self.params['W'+str(i)], self.params['b'+str(i)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            A = self.relu(Z)\n",
    "            cache['Z'+str(i)] = Z\n",
    "            cache['A'+str(i)] = A\n",
    "\n",
    "        # last layer\n",
    "        W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = self.sigmoid(Z)\n",
    "        cache['Z'+str(i+1)] = Z\n",
    "        cache['A'+str(i+1)] = A\n",
    "\n",
    "        return cache, A\n",
    "    \n",
    "    def backward(self, cache, X, Y):\n",
    "        \"\"\"\n",
    "        cache: result [A, Z]\n",
    "        Y: shape (1, m)\n",
    "        \"\"\"\n",
    "        grad = {}\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        m = Y.shape[1]\n",
    "        cache['A0'] = X\n",
    "\n",
    "        for l in range(n_layers, 0, -1):\n",
    "            A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "            W = self.params['W'+str(l)]\n",
    "            if l == n_layers:\n",
    "                dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "\n",
    "            if l == n_layers:\n",
    "                dZ = np.multiply(dA, self.sigmoid_grad(A, Z))\n",
    "            else:\n",
    "                dZ = np.multiply(dA, self.relu_grad(A, Z))\n",
    "            dW = np.dot(dZ, A_prev.T)/m\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "            dA = np.dot(W.T, dZ)\n",
    "\n",
    "            grad['dW'+str(l)] = dW\n",
    "            grad['db'+str(l)] = db\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def optimize(self, grads, lr):\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        for i in range(1, n_layers+1):\n",
    "            dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "            self.params['W'+str(i)] -= lr*dW\n",
    "            self.params['b'+str(i)] -= lr*db\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_batch(X, batch_size):\n",
    "        n = X.shape[0]\n",
    "        batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return batches\n",
    "    \n",
    "    \n",
    "    def train(self, X_train, y_train, batch_size=200, n_iter=100, lr=0.1):\n",
    "        # prepare batch training\n",
    "        batches = self.generate_batch(X_train, batch_size)\n",
    "        # init weights\n",
    "        self.weights_init()\n",
    "        for i in range(n_iter):\n",
    "            for batch in batches:\n",
    "                X = X_train[batch, :].T\n",
    "                Y = y_train[batch].reshape(1, -1)\n",
    "                cache, A = self.forward(X)\n",
    "                grads = self.backward(cache, X, Y)\n",
    "                self.optimize(grads, lr)\n",
    "\n",
    "            if i%10 == 0:\n",
    "                loss = self.compute_cost(A, Y)\n",
    "                print(f'iteration {i}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6930968966284916\n",
      "iteration 10: loss 0.6930261983198653\n",
      "iteration 20: loss 0.6930234151665605\n",
      "iteration 30: loss 0.6930149122135475\n",
      "iteration 40: loss 0.6929815230264361\n",
      "iteration 50: loss 0.6927740045307099\n",
      "iteration 60: loss 0.6880564419952588\n",
      "iteration 70: loss 0.2200907541999881\n",
      "iteration 80: loss 0.11582658029026635\n",
      "iteration 90: loss 0.08402195069870581\n"
     ]
    }
   ],
   "source": [
    "layers = [200, 20, 10, 1]\n",
    "model = deepNN(layers)\n",
    "\n",
    "model.train(X_train, y_train, batch_size=200, n_iter=100, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.55%\n"
     ]
    }
   ],
   "source": [
    "_, pred = model.forward(X_test.T)\n",
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
